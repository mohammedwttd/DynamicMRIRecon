# DynamicMRIRecon

[](https://opensource.org/licenses/MIT)
[](https://www.python.org/downloads/)
[](https://pytorch.org/)

This repository contains the official research code for "*(Your Paper Title Here)*," exploring dynamically adaptive neural networks for accelerated MRI reconstruction.

Our work investigates methods that move beyond static, fixed-cost architectures, allowing networks to adapt their computational depth and width based on scan difficulty.

## üöÄ Core Concepts

This project explores several novel ideas for dynamic reconstruction:

  * **Adaptive-Depth Cascades:** Implementing unrolled networks (e.g., Variational Networks, Recurrent Inference Machines) that can dynamically terminate early ("early-exit") or "grow" deeper on-the-fly for more challenging reconstructions.
  * **Dynamic Regularization Priors:** Implementing methods to grow, prune, and "relocate" neurons *within* the regularization network (e.g., the U-Net denoiser) during training.
  * **Gradient-Maximal Growing:** Using the **GradMax** principle to initialize newly added neurons/filters for maximal gradient norm, ensuring they contribute to loss reduction immediately.
  * **Dynamic Sparse Training:** Pruning "dormant" or low-impact neurons and re-allocating that computational budget to new, high-gradient neurons.

-----

## üîß Installation

To set up your environment, we recommend using Conda:

```bash
# Clone the repository
git clone https://github.com/YourUsername/DynamicMRIRecon.git
cd DynamicMRIRecon

# Create and activate the conda environment
conda env create -f environment.yml
conda activate dyn-mri
```

If you prefer `pip`:

```bash
pip install -r requirements.txt
```

-----

## ‚ö°Ô∏è Usage

### Data Setup

This project uses the [fastMRI](https://fastmri.med.nyu.edu/) dataset. Please download the dataset and organize it as follows:

```
/data/
  fastmri/
    knee_multicoil_train/
    knee_multicoil_val/
    ...
```

Update the `config/data.yaml` file with the correct path to your data root.

### Training a New Model

To train the baseline "Fixed-Depth" unrolled network:

```bash
python train.py --config config/fixed_depth_model.yaml
```

To train our proposed "Adaptive-Depth" model:

```bash
python train.py --config config/adaptive_depth_model.yaml
```

To train a model with "GradMax-Growing" regularization:

```bash
python train.py --config config/gradmax_growing_model.yaml
```

### Reconstructing an Image

To run inference on a validation or test file:

```bash
python reconstruct.py \
    --checkpoint /path/to/your/model.ckpt \
    --input_file /path/to/data/file.h5 \
    --output_path /results/
```

-----

## üìä Expected Results

Here, we will add qualitative and quantitative results, such as:

  * **[TODO]** A GIF comparing a fixed-depth model's reconstruction (left) vs. our adaptive-depth model (right) on a challenging, high-acceleration scan.
  * **[TODO]** A plot showing `SSIM vs. GFLOPs`, demonstrating how our adaptive models achieve higher quality at a lower computational cost for most scans.
  * **[TODO]** Visualization of a U-Net's channels being dynamically pruned and re-grown during training.

| Model | PSNR (dB) | SSIM | GFLOPs (Avg) |
| :--- | :--- | :--- | :--- |
| Fixed-Depth (10-iter) | 32.10 | 0.895 | 450.2 |
| **Adaptive-Depth (Ours)** | **32.08** | **0.894** | **180.5** |
| **GradMax-Growing (Ours)** | **32.25** | **0.901** | 460.1 |

-----

## üìù TODO / Future Research

  * [ ] Implement the "neuron relocation" (dynamic pruning and GradMax-regrowth) mechanism.
  * [ ] Test the adaptive-depth module on different backbones (VarNet, RIM).
  * [ ] Explore combining adaptive depth *and* adaptive width in a single model.
  * [ ] Open-source pre-trained model weights.

-----

## üìú Citation

If you use this work, please cite our paper:

```bibtex
@inproceedings{YourName2026Dynamic,
  title={Dynamic and Adaptive Networks for Accelerated MRI Reconstruction},
  author={Your Name, and Co-author Name},
  booktitle={MICCAI or ISMRM},
  year={2026}
}
```

## License

This project is licensed under the MIT License. See the [LICENSE](https://www.google.com/search?q=LICENSE) file for details.